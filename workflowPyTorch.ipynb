{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T05:06:32.106677800Z",
     "start_time": "2023-07-28T05:06:32.076815900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-28T05:06:32.184092400Z",
     "start_time": "2023-07-28T05:06:32.089253500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "([0.0, 0.02, 0.04, 0.06, 0.08, 0.1, 0.12, 0.14, 0.16, 0.18],\n [0.3,\n  0.314,\n  0.32799999999999996,\n  0.34199999999999997,\n  0.356,\n  0.37,\n  0.384,\n  0.398,\n  0.412,\n  0.426])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "X = np.arange(start,end, step)\n",
    "y = weight*X + bias\n",
    "X = list(X)\n",
    "y = list(y)\n",
    "X[:10], y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "weight = 0\n",
    "bias = 0\n",
    "def forward(x):\n",
    "    return x * weight + bias\n",
    "\n",
    "def backward(bias, weight):\n",
    "    weight = weight\n",
    "    bias = bias"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T05:06:32.186151700Z",
     "start_time": "2023-07-28T05:06:32.118023Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "## Implementing gradient descent and backpropagation without using torch\n",
    "def gradientDescent(x, y):\n",
    "    global bias, weight\n",
    "    learning_rate = 0.1\n",
    "    predicted_y = forward(x)\n",
    "    loss = ((y-predicted_y)**2)\n",
    "    print(f\"y : {y}, predicted y : {predicted_y}, loss : {loss}\")\n",
    "    loss_gradient = -2*(y-predicted_y)\n",
    "    nweight = loss_gradient*x\n",
    "    nbias = loss_gradient\n",
    "    bias = bias - learning_rate * nbias\n",
    "    weight = weight - learning_rate * nweight\n",
    "    #model.backward(bias,weight)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T05:06:32.186151700Z",
     "start_time": "2023-07-28T05:06:32.137871300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y : 0.3, predicted y : 0.0, loss : 0.09\n",
      "y : 0.314, predicted y : 0.06, loss : 0.064516\n",
      "y : 0.32799999999999996, predicted y : 0.11084064, loss : 0.04715818763560958\n",
      "y : 0.34199999999999997, predicted y : 0.1543970684928, loss : 0.03519485991009517\n",
      "y : 0.356, predicted y : 0.19215281910608692, loss : 0.02684589868688267\n",
      "y : 0.37, predicted y : 0.22528450097546152, loss : 0.020942575657921198\n",
      "y : 0.384, predicted y : 0.25472743927707586, loss : 0.01671139495586211\n",
      "y : 0.398, predicted y : 0.28122671472434735, loss : 0.013636000154068961\n",
      "y : 0.412, predicted y : 0.30537697442531736, loss : 0.011368469582699424\n",
      "y : 0.426, predicted y : 0.32765357953512303, loss : 0.00967201841825437\n",
      "y : 0.43999999999999995, predicted y : 0.3484370479593723, loss : 0.008383774186394279\n",
      "y : 0.45399999999999996, predicted y : 0.36803229187211683, loss : 0.007390446840760902\n",
      "y : 0.46799999999999997, predicted y : 0.3866838023838179, loss : 0.006612323994753977\n",
      "y : 0.482, predicted y : 0.4045876695247508, loss : 0.005992668909609197\n",
      "y : 0.496, predicted y : 0.42190112417267833, loss : 0.005490643398872835\n",
      "y : 0.51, predicted y : 0.43875013429689397, loss : 0.0050765433627106464\n",
      "y : 0.524, predicted y : 0.4552354694447937, loss : 0.004728560662477903\n",
      "y : 0.538, predicted y : 0.4714375561653389, loss : 0.004430558929242419\n",
      "y : 0.552, predicted y : 0.48742037658107695, loss : 0.00417052776092992\n",
      "y : 0.566, predicted y : 0.5032346077089517, loss : 0.003939494469449182\n",
      "y : 0.58, predicted y : 0.5189201566978809, loss : 0.0037307472578114137\n",
      "y : 0.594, predicted y : 0.5345082140946099, loss : 0.0035392725902127675\n",
      "y : 0.608, predicted y : 0.5500229214333887, loss : 0.0033613416391190163\n",
      "y : 0.622, predicted y : 0.5654827292380459, loss : 0.003194201894380034\n",
      "y : 0.6359999999999999, predicted y : 0.5809015056765203, loss : 0.003035844076714511\n",
      "y : 0.6499999999999999, predicted y : 0.5962894436561811, loss : 0.0028848238627625367\n",
      "y : 0.6639999999999999, predicted y : 0.6116538043513622, loss : 0.0027401241988854606\n",
      "y : 0.6779999999999999, predicted y : 0.6269995274379437, loss : 0.002601048201553047\n",
      "y : 0.692, predicted y : 0.6423297322270838, loss : 0.0024671355006331943\n",
      "y : 0.706, predicted y : 0.6576461290931053, loss : 0.0023380968316806367\n",
      "y : 0.72, predicted y : 0.6729493568063252, loss : 0.002213763024938497\n",
      "y : 0.734, predicted y : 0.6882392583958945, loss : 0.00209404547215771\n",
      "y : 0.748, predicted y : 0.7035151058113073, loss : 0.001978905810979188\n",
      "y : 0.762, predicted y : 0.7187757817901745, loss : 0.0018683330398506079\n",
      "y : 0.776, predicted y : 0.7340199258678446, loss : 0.0017623266241412656\n",
      "y : 0.79, predicted y : 0.7492460502969218, loss : 0.0016608844164010292\n",
      "y : 0.804, predicted y : 0.7644526307145123, loss : 0.0015639944174027393\n",
      "y : 0.8180000000000001, predicted y : 0.7796381756487445, loss : 0.0014716295675565848\n",
      "y : 0.8319999999999999, predicted y : 0.79480127835164, loss : 0.0013837448922721521\n",
      "y : 0.8459999999999999, predicted y : 0.8099406539496367, loss : 0.0013002764375798445\n",
      "y : 0.8599999999999999, predicted y : 0.8250551644903006, loss : 0.0012211415287999405\n",
      "y : 0.8739999999999999, predicted y : 0.8401438341142962, loss : 0.0011462399684802889\n",
      "y : 0.8879999999999999, predicted y : 0.8552058562806657, loss : 0.001075455862284347\n",
      "y : 0.9019999999999999, predicted y : 0.8702405947100212, loss : 0.0010086598243731252\n",
      "y : 0.9159999999999999, predicted y : 0.8852475794765244, loss : 0.0009457113680526763\n",
      "y : 0.9299999999999999, predicted y : 0.9002264994701519, loss : 0.0008864613338008629\n",
      "y : 0.944, predicted y : 0.9151771922621208, loss : 0.0008307542458947464\n",
      "y : 0.958, predicted y : 0.9300996322362041, loss : 0.0007784305213550612\n",
      "y : 0.972, predicted y : 0.9449939176956124, loss : 0.0007293284814313578\n",
      "y : 0.986, predicted y : 0.9598602575183146, loss : 0.0006832861370088273\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for x,y in zip(X,y):\n",
    "    gradientDescent(x,y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-28T05:06:32.187891800Z",
     "start_time": "2023-07-28T05:06:32.151803800Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
